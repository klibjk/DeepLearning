# -*- coding: utf-8 -*-
"""Copy of A1_starter_CAP6619.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQW4Py5TKN-CFD5bZqIKAva4p4T89ywT

# CAP 6619 - Deep Learning - Dr Marques
## Project 1
## Handwritten Digit Classifier Using the MNIST Dataset

Useful references and sources:

- https://www.tensorflow.org/datasets/catalog/mnist

- https://en.wikipedia.org/wiki/MNIST_database

- https://keras.io/examples/vision/mnist_convnet/

- https://github.com/the-deep-learners/deep-learning-illustrated/blob/master/notebooks/shallow_net_in_keras.ipynb

### (OPTIONAL) TODO 1
* https://stackoverflow.com/questions/60636444/what-is-the-difference-between-x-test-x-train-y-test-y-train-in-sklearn
* https://python-course.eu/machine-learning/training-and-testing-with-mnist.php
* https://www.youtube.com/watch?v=zyQVmq2rq_I&ab_channel=DataThinkers
* https://keras.io/api/optimizers/
* https://keras.io/api/losses/
* https://numpy.org/doc/stable/reference/generated/numpy.argmax.html
* https://machinelearningmastery.com/argmax-in-machine-learning/
* https://neptune.ai/blog/keras-metrics#:~:text=The%20confusion_matrix%20displays%20a%20table,and%20106%20%2B%20714%20wrong%20predictions
* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions
* https://stackoverflow.com/questions/47435526/what-is-the-meaning-of-axis-1-in-keras-argmax
* https://snyk.io/advisor/python/sklearn/functions/sklearn.metrics.confusion_matrix
* https://gist.github.com/RyanAkilos/3808c17f79e77c4117de35aa68447045?permalink_comment_id=3066704

Add your own sources and references here.

## Setup
"""

from tensorflow import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.optimizers import SGD

from tensorflow.keras import layers

from matplotlib import pyplot as plt

import numpy as np

"""## Load and prepare the data"""

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and validation sets
(X_train, y_train), (X_valid, y_valid) = mnist.load_data()

X_train.shape

y_train.shape

y_train[0:12]

plt.figure(figsize=(5,5))
for k in range(12):
    plt.subplot(3, 4, k+1)
    plt.imshow(X_train[k], cmap='Greys')
    plt.axis('off')
plt.tight_layout()
plt.show()

X_valid.shape

y_valid.shape

y_valid[0]

plt.imshow(X_valid[0], cmap='Greys')
plt.axis('off')
plt.show()

# Reshape (flatten) images
X_train_reshaped = X_train.reshape(60000, 784).astype('float32')
X_valid_reshaped = X_valid.reshape(10000, 784).astype('float32')

# studying shape
print("value of X_train_reshaped:\n")
print(X_train_reshaped[0])

# Scale images to the [0, 1] range
X_train_scaled_reshaped = X_train_reshaped / 255
X_valid_scaled_reshaped = X_valid_reshaped / 255

# studying shape
print("value of X_train_scaled_reshaped:\n")
print(X_train_scaled_reshaped[0])

# Renaming for conciseness
X_training = X_train_scaled_reshaped
X_validation = X_valid_scaled_reshaped

print("X_training shape (after reshaping + scaling):", X_training.shape)
print(X_training.shape[0], "train samples")
print("X_validation shape (after reshaping + scaling):", X_validation.shape)
print(X_validation.shape[0], "validation samples")

# convert class vectors to binary class matrices
y_training = keras.utils.to_categorical(y_train, num_classes)
y_validation = keras.utils.to_categorical(y_valid, num_classes)

print(y_valid[0])
print(y_validation[0])

"""## PART 1 - Shallow neural network architecture"""

model = Sequential()
model.add(Dense(64, activation='sigmoid', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))

model.summary()

(64*784)

(64*784)+64

(10*64)+10

"""### Configure model"""

model.compile(
    loss='mean_squared_error',
    optimizer=SGD(learning_rate=0.01),
    metrics=['accuracy']
)

"""### (OPTIONAL) TODO 2

Try different options for `loss` and `optimizer`, for example:
```
model.compile(
  optimizer='Adadelta',
  loss='categorical_crossentropy',
  metrics=['accuracy']
)
```
"""

#OPTIONAL Alternative

# from tensorflow.keras.optimizers import Adam
# model.compile(
#     loss='poisson',
#     optimizer=Adam(learning_rate=0.01),
#     metrics=['accuracy']
# )

"""### Train!"""

batch_size=128
epochs=500

history = model.fit(
  X_training, # training data
  y_training, # training targets
  epochs=epochs,
  batch_size=batch_size,
  verbose=1,
  validation_data=(X_validation, y_validation)
)

"""### Plot learning curves"""

# list all data in history
print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Evaluate the model"""

model.evaluate(X_validation, y_validation)

"""### TODO 3

Write code to display the confusion matrix for your classifier and comment on the insights such confusion matrix provides.

See [this](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) for an example.
"""

from sklearn.metrics import confusion_matrix

# Get predictions
y_pred = model.predict(X_validation)
y_pred_classes = np.argmax(y_pred, axis=1)
y_valid_classes = np.argmax(y_validation, axis=1)
# argmax with axis=1 is used because y_pred and y_validation are vectors that have the index of predicted/true labels across each row


# Compute confusion matrix
conf_mat = confusion_matrix(y_valid_classes, y_pred_classes)

# Print confusion matrix
print(conf_mat)

"""### (OPTIONAL) TODO 4

Write code to display 10 cases where the classifier makes mistakes. Make sure to display both the true value as well as the predicted value.



"""

# Code to display 10 cases where the classifier makes mistakes. The code displays both the true value as well as the predicted value.

# Code iterates through the validation set, and when it finds that the predicted value is different from the true value, it displays them. Stops at 10 mistakes.
mistakes = 0
for i in range(len(y_validation)):
    if np.argmax(y_validation[i]) != np.argmax(y_pred[i]):
        print(f"True value: {np.argmax(y_validation[i])}, Predicted value: {np.argmax(y_pred[i])}")

        mistakes += 1
        if mistakes == 10:
            break

"""TODO

QUESTION 1: Explain the meaning and contents of each of the following variables, X_training, y_training, X_validation, and y_validation.

X_training: It is a variable that is a NumPy array of shape (60000, 784) containing the training data in the MNIST dataset. This is the data that the model will use to iteratively learn how tune itself as it tries to find the global minimum of the loss function.

Each row of the array corresponds to an image. In the code the variable X_training is the result of reshapinng x_train from (60000, 28, 28) into (60000, 784). And the pixel values of the images get scaled from a 256 bit value to a value between 0 and 1.

y_training: NumPy array variable of shape (60000, 10) representing the training labels in the MINST dataset. This is the data that the model will use to evaluate itself during training.

Each row of the array corresponds to a one-hot encoded label vector for the corresponding image in X_training.

X_validation: This is a NumPy array variable of shape (10000, 784) containing the validation data in the MNIST dataset. This is the data that will be fed into the model after it has been trained, through the use of the predict() function.

y_validation: This variable is a NumPy array of shape (10000, 10) representing the true labels in the validation set for the MNIST dataset. Each row of the array corresponds to a one-hot encoded label vector for the corresponding image in X_validation. The variable is used to evaluate() the accuracy of the model.


QUESTION 2: Explain the meaning of the results you get when you run
model.summary() (see figure below).

https://stackoverflow.com/questions/36946671/keras-model-summary-result-understanding-the-of-parameters


Model: This is the type of the model.

Layer (type): This column lists the type of layer in the model in the order they are added to the model.

Output Shape: Column that lists the shape of the output of each layer.

Param #: Column that lists the number of parameters in each layer. For a dense layer, this is equal to the number of inputs multiplied by the number of neurons plus the number of biases. The biases are listed separately.

Total params: Total number of parameters in the model.

Trainable params: This is the number of parameters that are updated during training.

Non-trainable params: This is the number of parameters that are not updated during training.


QUESTION 3: Does the model show any indication of overfitting? Why (not)?

The accuracy and loss plots show that both the training and validation accuracy improve remain similar as they increase steadily over time. Additionally, the loss plots show that their loss also remains similar as they decrease steadily over time. This suggests that the model is not overfitting the training data.

## PART 2 - Convolutional neural network (CNN) architecture
"""

model_cnn = keras.Sequential(
    [
        keras.layers.InputLayer(input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

model_cnn.summary()

"""### Configure model"""

model_cnn.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

"""### Prepare the data
The CNN does not expect the images to be flattened.
"""

# Reload the data, just in case
(X_train, y_train), (X_valid, y_valid) = mnist.load_data()

# convert class vectors to binary class matrices
y_training = keras.utils.to_categorical(y_train, num_classes)
y_validation = keras.utils.to_categorical(y_valid, num_classes)

# Scale images to the [0, 1] range
X_train_cnn = X_train.astype("float32") / 255
X_valid_cnn = X_valid.astype("float32") / 255

# Redefine  dimension of train/test inputs
X_train_cnn = np.expand_dims(X_train_cnn, -1)
X_valid_cnn = np.expand_dims(X_valid_cnn, -1)

# Make sure images have shape (28, 28, 1)
print("x_train shape:", X_train_cnn.shape)
print(X_train_cnn.shape[0], "train samples")
print(X_valid_cnn.shape[0], "test samples")

"""### Train!

"""

batch_size=128
epochs=15

history = model_cnn.fit(
  X_train_cnn, # training data
  y_training, # training targets
  epochs=epochs,
  batch_size=batch_size,
  verbose=1,
  validation_data=(X_valid_cnn, y_validation)
)

"""### Plot learning curves"""

# list all data in history
print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Evaluate the model"""

model_cnn.evaluate(X_valid_cnn, y_validation)

"""TODO
QUESTION 4: How do the accuracy and loss compare to the previous model?
What can you infer from this comparison?

https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/

The accuracy and loss plots show that both the training and validation accuracy and loss start at very different magnitudes and seem to want to cross each other's paths by the last epochh. The behavior of the validation line suggest that overfitting because it seems to start going in the opposite accuracy/loss magnitude by the last epoch.

"""